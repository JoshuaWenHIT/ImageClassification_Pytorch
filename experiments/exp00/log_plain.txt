[2024-11-25 20:37:10] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet_preact
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/exp00
  log_period: 100
  checkpoint_period: 10
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2024-11-25 20:37:10] __main__ INFO: env_info:
  pytorch_version: 1.10.1
  cuda_version: 11.3
  cudnn_version: 8200
  num_gpus: 1
  gpu_name: NVIDIA GeForce RTX 3060 Laptop GPU
  gpu_capability: 8.6
[2024-11-25 20:37:51] __main__ INFO: MACs   : 255.25M
[2024-11-25 20:37:51] __main__ INFO: #params: 1.73M
[2024-11-25 20:37:51] __main__ INFO: Val 0
[2024-11-25 20:38:00] __main__ INFO: Epoch 0 loss 91749557.6576 acc@1 0.0970 acc@5 0.4964
[2024-11-25 20:38:00] __main__ INFO: Elapsed 9.21
[2024-11-25 20:38:00] __main__ INFO: Train 1 0
[2024-11-25 20:38:22] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 1.8876 (2.0287) acc@1 0.2969 (0.2502) acc@5 0.8828 (0.7778)
[2024-11-25 20:38:35] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 1.6447 (1.8785) acc@1 0.4219 (0.3026) acc@5 0.8516 (0.8270)
[2024-11-25 20:38:48] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 1.6457 (1.7891) acc@1 0.3672 (0.3354) acc@5 0.8828 (0.8500)
[2024-11-25 20:39:00] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 1.3905 (1.7214) acc@1 0.5078 (0.3611) acc@5 0.9297 (0.8661)
[2024-11-25 20:39:00] __main__ INFO: Elapsed 60.00
[2024-11-25 20:39:00] __main__ INFO: Val 1
[2024-11-25 20:39:09] __main__ INFO: Epoch 1 loss 1.4952 acc@1 0.4588 acc@5 0.9182
[2024-11-25 20:39:09] __main__ INFO: Elapsed 8.96
[2024-11-25 20:39:09] __main__ INFO: Train 2 390
[2024-11-25 20:39:29] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 1.3779 (1.3973) acc@1 0.5234 (0.4867) acc@5 0.9297 (0.9316)
[2024-11-25 20:39:41] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 1.1995 (1.3458) acc@1 0.5859 (0.5082) acc@5 0.9453 (0.9369)
[2024-11-25 20:39:54] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 1.1815 (1.2954) acc@1 0.5703 (0.5279) acc@5 0.9688 (0.9413)
[2024-11-25 20:40:06] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 1.2020 (1.2562) acc@1 0.6250 (0.5451) acc@5 0.9453 (0.9445)
[2024-11-25 20:40:06] __main__ INFO: Elapsed 57.38
[2024-11-25 20:40:06] __main__ INFO: Val 2
[2024-11-25 20:40:16] __main__ INFO: Epoch 2 loss 1.1963 acc@1 0.5825 acc@5 0.9520
[2024-11-25 20:40:16] __main__ INFO: Elapsed 9.33
[2024-11-25 20:40:16] __main__ INFO: Train 3 780
[2024-11-25 20:40:37] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.9291 (1.0297) acc@1 0.6406 (0.6347) acc@5 0.9844 (0.9633)
